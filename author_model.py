# -*- coding: utf-8 -*-
"""Author Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jkIA6TzPeyBfrCSbi5mocCzZITELh-XP
"""

import nltk
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os
# nltk.download('punkt')

model_path = "authorship_model.joblib"
vectorizer_path = "tfidf_vectorizer.joblib"

def load_model_and_vectorizer(model = model_path, vector = vectorizer_path):
    # Load the saved model
    classifier = joblib.load(model)
    # Load the saved TF-IDF vectorizer
    vectorizer = joblib.load(vector)
    return classifier, vectorizer

# Specify the folder path containing text files for each author
folder_path = './dataset/C50'

def read_files_from_folder(folder_path):
    data = {}
    for filename in os.listdir(folder_path):
        for file in os.listdir(folder_path+"/"+filename):
            if file.endswith(".txt"):
                author_name = filename
                file_path = os.path.join(folder_path+"/"+filename, file)
                with open(file_path, 'r', encoding='utf-8') as file:
                    data[author_name] = file.read()
    return data

def preprocess_text(text):
    # Tokenize the text into words
    tokens = nltk.word_tokenize(text.lower())
    # Remove punctuation and numbers
    tokens = [word for word in tokens if word.isalpha()]
    return ' '.join(tokens)

def train_authorship_model():
    data = read_files_from_folder(folder_path)
    # Preprocess the text data
    preprocessed_data = [(preprocess_text(text), author) for author, text in data.items()]
    # Split the dataset into training and testing sets
    train_data, test_data = train_test_split(preprocessed_data, test_size=0.2, random_state=42)
    # Extract features using TF-IDF vectorization
    vectorizer = TfidfVectorizer()
    X_train = vectorizer.fit_transform([item[0] for item in train_data])
    y_train = [item[1] for item in train_data]
    # Train a Naive Bayes classifier
    classifier = MultinomialNB()
    classifier.fit(X_train, y_train)
    # Save the trained model
    joblib.dump(classifier, 'authorship_model.joblib')
    joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')

def predict_author(input_text,top_n=5):
    # Preprocess the text
    preprocessed_text = preprocess_text(input_text)
    # Transform the text using the loaded vectorizer
    classifier, vectorizer = load_model_and_vectorizer()
    X = vectorizer.transform([preprocessed_text])

    # Get prediction probabilities for each class
    author = classifier.predict(X)[0]

    return author


# Example usage:
authors_data = {
    'Author1': 'This is the text written by Author1.',
    'Author2': 'Author2 expresses thoughts in this text.',
    'Author3': 'Text authored by Author3 is unique.'
}





#
# # Predict the author of the input paragraph
input_paragraph = '''Investors smiled on the bourses of central and Ljubljana was the one gray cloud, posting slight losses.
The Central European Share Ind
All-time high: CESI 1,483.76 (Ju0.53 (July 8/1996); PX50 1,002.4 (April 7/1994); RABSI 112.2 (April 27/1994); SAX 402.3 (Feb/1994).
'''
# predicted_author = predict_author(input_paragraph, classifier = model_path, vectorizer=vectorizer_path)
# print(f"The predicted author is: {predicted_author}")


# def plot_bar_chart(predictions):
#     authors = list(predictions.keys())
#     probabilities = list(predictions.values())
#     plt.bar(authors, probabilities, color='blue')
#     plt.xlabel('Authors')
#     plt.ylabel('Probabilities')
#     plt.title('Top 5 Author Prediction Probabilities')
#     plt.show()
#
# plot_bar_chart(predicted_author)

